{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from dateutil.parser import *\n",
    "import textract\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files in the folder:\n",
      "RED BULL CHERRY FLAVOR LAUNCH.docx\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'C:\\Users\\karth\\Desktop\\Capstone\\NER\\Data\\DealSheets'\n",
    "all_files = os.listdir(folder_path)\n",
    "all_pdfs = [file for file in all_files if file.endswith('.pdf') or file.endswith('.docx')]\n",
    "print(\"PDF files in the folder:\")\n",
    "for pdf in all_pdfs:\n",
    "    print(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import docx\n",
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = PyPDF2.PdfFileReader(f)\n",
    "        for page_num in range(reader.numPages):\n",
    "            text += reader.getPage(page_num).extractText()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file):\n",
    "    doc = docx.Document(file)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                text += cell.text + \"\\t\"\n",
    "            text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace \"\\r\\n\" with spaces\n",
    "    text = text.replace(\"\\r\\n\", \" \")\n",
    "    # Remove any double spaces\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def Clean_Text(file):\n",
    "    if file.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file)\n",
    "    elif file.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    \n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import docx\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    with fitz.open(file) as pdf_file:\n",
    "        for page_num in range(len(pdf_file)):\n",
    "            text += pdf_file[page_num].get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file):\n",
    "    doc = docx.Document(file)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def Get_Invoice_Number(file):\n",
    "    if file.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file)\n",
    "    elif file.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    \n",
    "    # Preprocess the text\n",
    "    text = text.replace(\"\\r\\n\", \"|\")\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    tokens = [i for i in text.split('|') if len(i) > 1]\n",
    "    text = '|'.join(tokens)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(\" number:| no\\.| n\\.\", \" number\", text)\n",
    "\n",
    "    # Extract the Invoice Number\n",
    "    valid_matches = []\n",
    "    keywords = [\"invoice number\", \"lading number\", \"invoice\", \"lading\", \"number\"]\n",
    "    for keyword in keywords:\n",
    "        token = Extract_Tag_Data(keyword, text, 'reverse')\n",
    "        if len(token) > 0:\n",
    "            valid_matches.append(token)\n",
    "\n",
    "    valid_matches = [i.upper() for i in valid_matches]\n",
    "\n",
    "    if len(valid_matches) > 0:\n",
    "        invoice_number = valid_matches[0]\n",
    "        if len(invoice_number) > 20:\n",
    "            invoice_number = invoice_number.split('/')[0]\n",
    "    else:\n",
    "        invoice_number = \"No Invoice Number found\"\n",
    "    \n",
    "    return invoice_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Deal_Dates(text):\n",
    "    possible_dates = []\n",
    "\n",
    "    # Extract entities labeled as \"DATE\"\n",
    "    doc = nlp(text)\n",
    "    possible_dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "\n",
    "    # Extract start and end dates from possible dates\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    for date in possible_dates:\n",
    "        try:\n",
    "            parsed_date = parse(date)\n",
    "            if start_date is None or parsed_date < start_date:\n",
    "                start_date = parsed_date\n",
    "            if end_date is None or parsed_date > end_date:\n",
    "                end_date = parsed_date\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if start_date and end_date:\n",
    "        return start_date.strftime(\"%B %d, %Y\"), end_date.strftime(\"%B %d, %Y\")\n",
    "    else:\n",
    "        return \"No Start and End Dates Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_collaborators(text):\n",
    "    collaborators = {}\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    for line_index, line in enumerate(lines):\n",
    "        if \"Name:\" in line:\n",
    "            name_index = line.index(\"Name:\")\n",
    "            name = line[name_index + len(\"Name:\"):].strip()\n",
    "            if name:\n",
    "                if line_index < len(lines) - 1:\n",
    "                    address = lines[line_index + 1].strip()\n",
    "                    collaborators[name] = address\n",
    "\n",
    "    return collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_integrations(text):\n",
    "#     integrations = []\n",
    "\n",
    "#     # Find all occurrences of the keyword \"Integrations\" (case-insensitive)\n",
    "#     integration_matches = re.finditer(r'(\\w+\\s+Integrations):\\s*\\n', text, re.IGNORECASE)\n",
    "\n",
    "#     for match in integration_matches:\n",
    "#         integration_type = match.group(1)\n",
    "#         # integration_section = re.search(rf'{integration_type}\\s*:(.*?)\\n[ivx]{1,3}\\.', text, re.IGNORECASE | re.DOTALL)\n",
    "#         integrations.append(integration_type)\n",
    "#         # if integration_section:\n",
    "#         #     integration_elements = re.findall(r'Element\\s+(.*?)\\n', integration_section.group(1), re.IGNORECASE)\n",
    "#         #     integrations[integration_type] = [element.strip() for element in integration_elements]\n",
    "\n",
    "#     return integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_integrations(file_text):\n",
    "    integrations = {}\n",
    "\n",
    "    # Define integration types to look for\n",
    "    integration_types = [\"In-Store Integrations\", \"Car Display Integrations\", \"Online Integrations\"]\n",
    "\n",
    "    for integration_type in integration_types:\n",
    "        start_index = file_text.find(integration_type)\n",
    "        if start_index != -1:\n",
    "            next_integration_index = len(file_text)\n",
    "            for next_type in integration_types:\n",
    "                if next_type != integration_type:\n",
    "                    next_start_index = file_text.find(next_type, start_index + len(integration_type))\n",
    "                    if next_start_index != -1 and next_start_index < next_integration_index:\n",
    "                        next_integration_index = next_start_index\n",
    "            integrations[integration_type] = file_text[start_index:next_integration_index].strip()\n",
    "\n",
    "    return integrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import os\n",
    "from spire.doc import *\n",
    "from spire.doc.common import *\n",
    "\n",
    "def extract_images_from_docx(input_file, output_path):\n",
    "    # Create a Document instance\n",
    "    document = Document()\n",
    "    # Load the input Word document\n",
    "    document.LoadFromFile(input_file)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Create a list to store the extracted image data\n",
    "    images = []\n",
    "\n",
    "    # Initialize a queue to store document elements for traversal\n",
    "    nodes = queue.Queue()\n",
    "    nodes.put(document)\n",
    "\n",
    "    # Traverse through the document elements\n",
    "    while not nodes.empty():\n",
    "        node = nodes.get()\n",
    "        for i in range(node.ChildObjects.Count):\n",
    "            obj = node.ChildObjects[i]\n",
    "            # Find the images\n",
    "            if isinstance(obj, DocPicture):\n",
    "                picture = obj\n",
    "                # Append the image data to the list\n",
    "                data_bytes = picture.ImageBytes\n",
    "                images.append(data_bytes)\n",
    "            elif isinstance(obj, ICompositeObject):\n",
    "                nodes.put(obj)\n",
    "\n",
    "    # Save the image data to image files\n",
    "    for i, image_data in enumerate(images):\n",
    "        file_name = f\"Image-{i}.png\"\n",
    "        with open(os.path.join(output_path, file_name), 'wb') as image_file:\n",
    "            image_file.write(image_data)\n",
    "\n",
    "    document.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File : C:\\Users\\karth\\Desktop\\Capstone\\NER\\Data\\DealSheets\\RED BULL CHERRY FLAVOR LAUNCH.docx\n",
      "The collaborators\n",
      "1-> Red Bull GmbH\n",
      "    Address: Address: Am Brunnen 1, 5330 Fuschl am See, Austria\n",
      "2-> Costco Wholesale Corporation\n",
      "    Address: Address: 999 Lake Drive, Issaquah, WA 98027, USA\n",
      "\n",
      "\n",
      "Start Date : January 01, 2024\n",
      "End Date : March 01, 2024\n",
      "\n",
      "\n",
      "Integrations:\n",
      "In-Store Integrations\n",
      "Car Display Integrations\n",
      "Online Integrations\n"
     ]
    }
   ],
   "source": [
    "for file in all_pdfs:\n",
    "\n",
    "    file = folder_path+\"\\\\\"+file\n",
    "    print(\"Processing File :\", file)\n",
    "    \n",
    "    file_text = Clean_Text(file)\n",
    "    Date = Get_Deal_Dates(file_text)\n",
    "    collaborators = extract_collaborators(file_text)\n",
    "    \n",
    "    print(\"The collaborators\")\n",
    "    i=0\n",
    "    for x in collaborators.keys():\n",
    "        i+=1\n",
    "        print(str(i)+\"->\",x)\n",
    "        print(\"    Address:\",collaborators[x])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Start Date :\", Date[0])\n",
    "    print(\"End Date :\", Date[1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    integrations = extract_integrations(file_text)\n",
    "\n",
    "    print(\"Integrations:\")\n",
    "    for x in integrations:\n",
    "        print(x)\n",
    "    \n",
    "    \n",
    "    extract_images_from_docx(file,'./Data/DocumentImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# def image_to_hex(image_path):\n",
    "#     img = Image.open(image_path)\n",
    "#     img = img.resize((100, 100))  # Resize the image if needed\n",
    "#     img = img.convert('L')  # Convert to grayscale if needed\n",
    "#     hex_representation = img.tobytes().hex()\n",
    "#     return hex_representation\n",
    "\n",
    "# def process_images(folder_path, output_file):\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for filename in os.listdir(folder_path):\n",
    "#             if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 image_path = os.path.join(folder_path, filename)\n",
    "#                 hex_value = image_to_hex(image_path)\n",
    "#                 f.write(f\"{filename}: {hex_value}\\n\")\n",
    "\n",
    "# folder_path = './Data/DocumentImages'\n",
    "# output_file = './Data/hex_values.txt'\n",
    "# process_images(folder_path, output_file)\n",
    "# print(\"Hex values saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the images to hex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image hashes saved to: ./Data/hex_values.txt\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import imagehash\n",
    "import os\n",
    "\n",
    "def calculate_image_hash1(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    return str(imagehash.average_hash(img))\n",
    "\n",
    "def process_images1(folder_path, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image_hash = calculate_image_hash1(image_path)\n",
    "                f.write(f\"{filename}: {image_hash}\\n\")\n",
    "\n",
    "folder_path = './Data/DocumentImages'\n",
    "output_file = './Data/hex_values.txt'\n",
    "process_images1(folder_path, output_file)\n",
    "print(\"Image hashes saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hex values stored in Redis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import redis\n",
    "\n",
    "def image_to_hex(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    return str(imagehash.average_hash(img))\n",
    "\n",
    "def process_images(folder_path, redis_host, redis_port, redis_db):\n",
    "    r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            hex_value = image_to_hex(image_path)\n",
    "            # Store hex value in Redis with filename as key\n",
    "            r.set(hex_value, filename)\n",
    "\n",
    "# Example usage\n",
    "folder_path = './Data/DocumentImages'\n",
    "redis_host = 'localhost'  # Replace with your Redis server host\n",
    "redis_port = 6379  # Replace with your Redis server port\n",
    "redis_db = 0  # Replace with your Redis database index\n",
    "process_images(folder_path, redis_host, redis_port, redis_db)\n",
    "print(\"Hex values stored in Redis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 7cfce21282e0f8fe, Value: Image-1.png\n",
      "Key: ffa7e7e3c3c7c7ff, Value: Image-0.png\n"
     ]
    }
   ],
   "source": [
    "def display_all_keys(redis_host, redis_port, redis_db):\n",
    "    r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)\n",
    "    keys = r.keys()\n",
    "    for key in keys:\n",
    "        value = r.get(key).decode('utf-8')  # Decode bytes to string\n",
    "        print(f\"Key: {key.decode('utf-8')}, Value: {value}\")\n",
    "        \n",
    "display_all_keys(redis_host, redis_port, redis_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To delet all the keys from the Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys deleted from the Redis database.\n"
     ]
    }
   ],
   "source": [
    "# def delete_all_keys(redis_host, redis_port, redis_db):\n",
    "#     r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)\n",
    "#     r.flushdb()\n",
    "#     print(\"All keys deleted from the Redis database.\")\n",
    "    \n",
    "# delete_all_keys(redis_host, redis_port, redis_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the image is duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Image-0.png is duplicated.\n",
      "Image Image-1.png is duplicated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import redis\n",
    "\n",
    "def image_to_hex(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    return str(imagehash.average_hash(img))\n",
    "\n",
    "def process_images(folder_path, redis_host, redis_port, redis_db):\n",
    "    r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            hex_value = image_to_hex(image_path)\n",
    "            # Check if the key already exists in Redis\n",
    "            if r.exists(hex_value):\n",
    "                print(f\"Image {filename} is duplicated.\")\n",
    "            else:\n",
    "                # Store hex value in Redis with filename as key\n",
    "                r.set(hex_value, filename)\n",
    "\n",
    "# Example usage\n",
    "folder_path = './Data/DocumentImages'\n",
    "redis_host = 'localhost'  # Replace with your Redis server host\n",
    "redis_port = 6379  # Replace with your Redis server port\n",
    "redis_db = 0  # Replace with your Redis database index\n",
    "process_images(folder_path, redis_host, redis_port, redis_db)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
